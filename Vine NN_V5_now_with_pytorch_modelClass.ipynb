{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Laden der Daten   --- NOCH UNFERTIG!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    },
    {
     "data": {
      "text/plain": "   malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  flavanoids  \\\n0        1.71  2.43               15.6      127.0           2.80        3.06   \n1        1.78  2.14               11.2      100.0           2.65        2.76   \n2        2.36  2.67               18.6      101.0           2.80        3.24   \n3        1.95  2.50               16.8      113.0           3.85        3.49   \n4        2.59  2.87               21.0      118.0           2.80        2.69   \n\n   nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0                  0.28             2.29             5.64  1.04   \n1                  0.26             1.28             4.38  1.05   \n2                  0.30             2.81             5.68  1.03   \n3                  0.24             2.18             7.80  0.86   \n4                  0.39             1.82             4.32  1.04   \n\n   od280/od315_of_diluted_wines  proline  \n0                          3.92   1065.0  \n1                          3.40   1050.0  \n2                          3.17   1185.0  \n3                          3.45   1480.0  \n4                          2.93    735.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vine dataset from scikit-learn\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "# import train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "\n",
    "print(data.feature_names)\n",
    "\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df.head()\n",
    "\n",
    "y = df['alcohol']\n",
    "df = df.drop(\"alcohol\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scaling der Daten"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n0 -0.562250    0.232053 -1.169593           1.913905   0.808997   \n1 -0.499413   -0.827996 -2.490847           0.018145   0.568648   \n2  0.021231    1.109334 -0.268738           0.088358   0.808997   \n3 -0.346811    0.487926 -0.809251           0.930918   2.491446   \n4  0.227694    1.840403  0.451946           1.281985   0.808997   \n\n   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n0       1.034819   -0.659563              1.224884         0.251717   \n1       0.733629   -0.820719             -0.544721        -0.293321   \n2       1.215533   -0.498407              2.135968         0.269020   \n3       1.466525   -0.981875              1.032155         1.186068   \n4       0.663351    0.226796              0.401404        -0.319276   \n\n   color_intensity       hue  od280/od315_of_diluted_wines  \n0         0.362177  1.847920                      1.013009  \n1         0.406051  1.113449                      0.965242  \n2         0.318304  0.788587                      1.395148  \n3        -0.427544  1.184071                      2.334574  \n4         0.362177  0.449601                     -0.037874  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.562250</td>\n      <td>0.232053</td>\n      <td>-1.169593</td>\n      <td>1.913905</td>\n      <td>0.808997</td>\n      <td>1.034819</td>\n      <td>-0.659563</td>\n      <td>1.224884</td>\n      <td>0.251717</td>\n      <td>0.362177</td>\n      <td>1.847920</td>\n      <td>1.013009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.499413</td>\n      <td>-0.827996</td>\n      <td>-2.490847</td>\n      <td>0.018145</td>\n      <td>0.568648</td>\n      <td>0.733629</td>\n      <td>-0.820719</td>\n      <td>-0.544721</td>\n      <td>-0.293321</td>\n      <td>0.406051</td>\n      <td>1.113449</td>\n      <td>0.965242</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.021231</td>\n      <td>1.109334</td>\n      <td>-0.268738</td>\n      <td>0.088358</td>\n      <td>0.808997</td>\n      <td>1.215533</td>\n      <td>-0.498407</td>\n      <td>2.135968</td>\n      <td>0.269020</td>\n      <td>0.318304</td>\n      <td>0.788587</td>\n      <td>1.395148</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.346811</td>\n      <td>0.487926</td>\n      <td>-0.809251</td>\n      <td>0.930918</td>\n      <td>2.491446</td>\n      <td>1.466525</td>\n      <td>-0.981875</td>\n      <td>1.032155</td>\n      <td>1.186068</td>\n      <td>-0.427544</td>\n      <td>1.184071</td>\n      <td>2.334574</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.227694</td>\n      <td>1.840403</td>\n      <td>0.451946</td>\n      <td>1.281985</td>\n      <td>0.808997</td>\n      <td>0.663351</td>\n      <td>0.226796</td>\n      <td>0.401404</td>\n      <td>-0.319276</td>\n      <td>0.362177</td>\n      <td>0.449601</td>\n      <td>-0.037874</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "df = pd.DataFrame(df, columns=data.feature_names[:-1])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split der daten und Konvertierung in Tensoren"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train.values, dtype=torch.float)\n",
    "x_test = torch.tensor(x_test.values, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "y_test= torch.tensor(y_test.values, dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([142, 12])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([142])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Überlegungen zur Netzarchitektur\n",
    "\n",
    "* Wir haben 12 Attribute / Spalten / Features\n",
    "* Wir haben 142 Traingsbeispiele\n",
    "\n",
    "Angenommen, zwei HiddenLayer, erstmal 70, dann 30 Neuronen\n",
    "* w1: von 12 auf 70\n",
    "* w2: von 70 auf 30\n",
    "* w3: von 30 auf 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5004, -0.5004,  1.5000, -0.4992])\n"
     ]
    }
   ],
   "source": [
    "def batch_norm(x):\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    return (x - mean) / std\n",
    "\n",
    "# Was bewirkt batch norm?\n",
    "# --> Normalisierung der Outputs der Neuronen\n",
    "# --> Wirkt sich positiv auf das Training aus\n",
    "\n",
    "# Warum?\n",
    "# --> Wirkt sich positiv auf die Gradienten aus, da diese nicht mehr so starke Schwankungen haben\n",
    "# vorher [0.2, 0.7, 213123.2, 123.3]\n",
    "vorher=torch.tensor([0.2, 0.7, 213123.2, 123.3])\n",
    "print(batch_norm(vorher))\n",
    "# nachher: [-0.5004, -0.5004,  1.5000, -0.4992]\n",
    "\n",
    "# Wie?\n",
    "# --> Durch die Normalisierung der Outputs der Neuronen, werden die Outputs der Neuronen in einem ähnlichen Bereich liegen\n",
    "\n",
    "# Wie wird das erreicht?\n",
    "# --> Durch die Subtraktion des Mittelwerts und die Division durch die Standardabweichung. Das Ergebnis ist eine Standardnormalverteilung. Dies ist eine Normalverteilung mit Mittelwert 0 und Standardabweichung 1.\n",
    "\n",
    "# In der Folge können auch tiefere Netze trainiert werden, da die Gradienten nicht mehr so stark schwanken. Außerdem wird das Vanishing Gradient Problem verhindert.\n",
    "\n",
    "def forward(x):\n",
    "    a2= torch.relu(x @ w1 + b1)\n",
    "    # batch norm\n",
    "    a2 = batch_norm(a2)   # 70 Neuronen, bzw. ein Tensor mit Shape(70,1)\n",
    "    a3 = torch.relu(a2 @ w2 + b2)\n",
    "    # batch norm\n",
    "    a3 = batch_norm(a3)   # 30 Neuronen\n",
    "    a4 = torch.relu(a3 @ w3 + b3)\n",
    "    y_hat = a4\n",
    "    return y_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modell bauen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class MySimpleNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MySimpleNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(12, 70)\n",
    "        self.fc2 = torch.nn.Linear(70, 120)\n",
    "        self.fc3 = torch.nn.Linear(120, 30)\n",
    "        self.fc4 = torch.nn.Linear(30, 1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(70)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(120)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a2 = torch.relu(self.fc1(x))    # dahinter steckt x@w1 + b1\n",
    "        a2_bn = self.bn1(a2)\n",
    "        a3 = torch.relu(self.fc2(a2_bn))\n",
    "        a3_bn = self.bn2(a3)\n",
    "        a4 = torch.relu(self.fc3(a3_bn))\n",
    "        a4_bn = self.bn3(a4)\n",
    "        y_hat = torch.relu(self.fc3(a4_bn))\n",
    "        return y_hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b60e33ec1ebb4fe68c6e53b469ca7101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 70])",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m     loss_sum\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(x_train)):\n\u001B[1;32m---> 20\u001B[0m         loss_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss_sum\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(x_train))\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#plot\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[16], line 5\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(single_x, single_y)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(single_x, single_y):\n\u001B[1;32m----> 5\u001B[0m     y_hat\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msingle_x\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     loss \u001B[38;5;241m=\u001B[39m (y_hat \u001B[38;5;241m-\u001B[39m single_y)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[0;32m      7\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\anaconda3_neu\\envs\\torch_etc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[13], line 13\u001B[0m, in \u001B[0;36mMySimpleNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     12\u001B[0m     a2 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(x))    \u001B[38;5;66;03m# dahinter steckt x@w1 + b1\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m     a2_bn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbn1\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     a3 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(a2_bn))\n\u001B[0;32m     15\u001B[0m     a3_bn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(a3)\n",
      "File \u001B[1;32m~\\anaconda3_neu\\envs\\torch_etc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3_neu\\envs\\torch_etc\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    164\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3_neu\\envs\\torch_etc\\lib\\site-packages\\torch\\nn\\functional.py:2448\u001B[0m, in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2435\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m   2436\u001B[0m         batch_norm,\n\u001B[0;32m   2437\u001B[0m         (\u001B[38;5;28minput\u001B[39m, running_mean, running_var, weight, bias),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2445\u001B[0m         eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[0;32m   2446\u001B[0m     )\n\u001B[0;32m   2447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[1;32m-> 2448\u001B[0m     \u001B[43m_verify_batch_size\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2450\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbatch_norm(\n\u001B[0;32m   2451\u001B[0m     \u001B[38;5;28minput\u001B[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001B[38;5;241m.\u001B[39mbackends\u001B[38;5;241m.\u001B[39mcudnn\u001B[38;5;241m.\u001B[39menabled\n\u001B[0;32m   2452\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3_neu\\envs\\torch_etc\\lib\\site-packages\\torch\\nn\\functional.py:2416\u001B[0m, in \u001B[0;36m_verify_batch_size\u001B[1;34m(size)\u001B[0m\n\u001B[0;32m   2414\u001B[0m     size_prods \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m size[i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m   2415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_prods \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 2416\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected more than 1 value per channel when training, got input size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(size))\n",
      "\u001B[1;31mValueError\u001B[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 70])"
     ]
    }
   ],
   "source": [
    "model = MySimpleNN()\n",
    "lr=0.0001\n",
    "\n",
    "def train(single_x, single_y):\n",
    "    y_hat=model(single_x)\n",
    "    loss = (y_hat - single_y)**2\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= lr * param.grad\n",
    "        model.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "losses=[]\n",
    "# import tqdm notebook\n",
    "from tqdm.notebook import tqdm\n",
    "for epoch in tqdm(range(50)):\n",
    "    loss_sum=0\n",
    "    for i in range(len(x_train)):\n",
    "        loss_sum += train(x_train[i], y_train[i])\n",
    "    losses.append(loss_sum/len(x_train))\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Auswertung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "errorSum=0\n",
    "for i in range(len(x_test)):\n",
    "    y_hat=forward(x_test[i])\n",
    "    y = y_test[i]\n",
    "    errorSum += torch.abs(y_hat - y)\n",
    "\n",
    "print(\"mean absolute error:\" , errorSum/len(x_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    y_hat=forward(x_test[i])\n",
    "    y = y_test[i]\n",
    "    print(\"y_hat: \", y_hat, \"y: \", y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
